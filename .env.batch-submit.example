######################################
# General
######################################

# AWS environment name (e.g. dev, stage, prod).
# Used when submitting AWS Batch jobs.
ENV="dev"

# S3 bucket used for job input and output.
BUCKET_NAME="bucket-name"

# OpenSearch domain host (without protocol).
OPENSEARCH_HOST="opensearch-domain.region.es.amazonaws.com"

# AWS region where the OpenSearch cluster is hosted.
OPENSEARCH_REGION="aws-region"

######################################
# Dataset Subset (Works)
######################################

# Enable or disable creation of a dataset subset for Works.
# Subsets can be filtered by institutions and/or a list of DOIs.
DATASET_SUBSET_ENABLE=true

# S3 path (excluding bucket name) to a JSON file containing ROR IDs
# and institution names. Only works authored by these institutions
# will be included.
DATASET_SUBSET_INSTITUTIONS_S3_PATH="meta/institutions.json"

# S3 path (excluding bucket name) to a JSON file containing specific
# Work DOIs to include in the subset.
DATASET_SUBSET_DOIS_S3_PATH="meta/work_dois.json"

######################################
# Dataset Subset Settings
######################################

# Enable or disable creation of a dataset subset for DMPs.
# Filtering is applied during the DMP works search.
DMP_SUBSET_ENABLE=true

# S3 path (excluding bucket name) to a JSON file containing ROR IDs
# and institution names. Only DMPs created by these institutions
# will be included.
DMP_SUBSET_INSTITUTIONS_S3_PATH="meta/dmp_institutions.json"

# S3 path (excluding bucket name) to a JSON file containing specific
# DMP DOIs to include in the subset.
DMP_SUBSET_DOIS_S3_PATH="meta/dmp_dois.json"

######################################
# ROR
######################################

# Identifier for this ROR ingestion run.
# Typically a date (YYYY-MM-DD) matching the dataset release.
ROR_RUN_ID=2016-01-29

# URL to download the ROR dataset from Zenodo.
ROR_DOWNLOAD_URL=https://zenodo.org/records/18419061/files/v2.2-2026-01-29-ror-data.zip

# Expected MD5 checksum of the ROR archive.
ROR_HASH=md5:26c6b5936fc5ca8716c56df5e7408c87

# Name of the extracted ROR file saved to S3.
# Currently set manually, but should be populated automatically.
ROR_FILE_NAME=v2.2-2026-01-29-ror-data.json.gz

######################################
# Crossref Metadata
######################################

# S3 bucket containing Crossref Metadata snapshots.
CROSSREF_METADATA_BUCKET_NAME=api-snapshots-reqpays-crossref

# Identifier for this Crossref ingestion run.
# Typically a date (YYYY-MM-DD) matching the dataset release.
CROSSREF_METADATA_RUN_ID=2025-04-01

# Name of the Crossref metadata archive to download.
# Currently set manually, but should be populated automatically.
CROSSREF_METADATA_FILE_NAME=April_2025_Public_Data_File_from_Crossref.tar

######################################
# DataCite
######################################

# S3 bucket containing DataCite monthly data files.
DATACITE_BUCKET_NAME=monthly-datafile.datacite.org

# Identifier for this DataCite ingestion run.
# Typically a date (YYYY-MM-DD) matching the dataset release.
DATACITE_RUN_ID=YYYY-MM-DD

######################################
# OpenAlex Works
######################################

# S3 bucket containing OpenAlex data.
OPENALEX_BUCKET_NAME=openalex

# Identifier for this OpenAlex Works ingestion run.
# Typically a date (YYYY-MM-DD) matching the dataset release.
OPENALEX_WORKS_RUN_ID=2026-02-03

# Maximum number of parallel file processes when normalising OpenAlex Works.
OPENALEX_MAX_FILE_PROCESSES=4

# Number of OpenAlex files processed per batch.
OPENALEX_BATCH_SIZE=4

######################################
# OpenAlex Funders
######################################

# Identifier for this OpenAlex Funders ingestion run.
# Typically a date (YYYY-MM-DD) matching the dataset release.
OPENALEX_FUNDERS_RUN_ID=2026-02-03

######################################
# Process Works
######################################

# Identifier for the previous processing run
# Used during SQLMesh transformation
# Typically a date (YYYY-MM-DD).
PREV_PROCESS_WORKS_RUN_ID=2026-01-01

# Identifier for this processing run of unified works.
# Typically a date (YYYY-MM-DD).
PROCESS_WORKS_RUN_ID=2026-02-04

# Number of threads DuckDB is allowed to use.
SQLMESH_DUCKDB_THREADS=32

# Memory limit for DuckDB.
SQLMESH_DUCKDB_MEMORY_LIMIT=200GB

# Maximum number of parallel processes used when syncing works to OpenSearch.
SYNC_MAX_PROCESSES=2

######################################
# Process DMPs
######################################

# Identifier for this DMP processing run.
# Typically a date (YYYY-MM-DD).
DMPS_RUN_ID=2026-02-04